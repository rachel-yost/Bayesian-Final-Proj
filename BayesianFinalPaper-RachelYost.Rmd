---
title: "R Notebook"
output: html_notebook
---


#Set up Data

```{r}
#libraries
library(gam)
library(pROC)

library(bayesrules)
library(rstanarm)
library(bayesplot)
library(tidyverse)
library(tidybayes)
library(broom.mixed)
library(survival)
library(Bolstad2)
library(knitr)

library(brms)
library(mgcv)
library(rstan)

```

```{r}
#read in data
fire <- read.csv("Fire_data.csv")
```



```{r}
#Histogram of Fire Weather Index - right skewed
hist(fire$FWI)

#Histogram of Initial Spread Index - right skewed
hist(fire$ISI)

#Histogram of # of Fires
hist(fire$number_of_fire)

#Number of Fires dont differ much between AM and PM, PM has slightly more fires
table(fire$Period)

#Initial Attack Size is zero inflated
hist(fire$IA_Size)

#Being Held Size is zero inflated
hist(fire$BH_Size)

#Histogram of Gap between initial and being held
hist(fire$Gap_BHIA)

#Table of Fuel Types - Most are C2
table(fire$Fuel_type)

#Table of Detection Type
table(fire$Detection)

#Histogram of fire response time - most are very quick
hist(fire$Response_time)

#histogram of months - most fires in june and then it goes down through september
hist(fire$Month)

#table of method - most air (615), followed by "other ground"(135)
table(fire$Method)

#histogram of Gap_UCIA
hist(fire$Gap_UCIA)

#hist of Ex_Size - most close to zero
hist(fire$Ex_Size)

#hist of GAP
hist(fire$Gap)

#table growth- most grew (629) vs 260
table(fire$Growth)


kable(quantile(fire$IA_Size),caption = "quantile-initial attack size")
kable(quantile(fire$BH_Size), ,caption = "quantile-being held size")
```

### Data preparation

We split the data 70/30 into a training and test set. 

#Implement Classification - Logistic Regression

```{r}
#Split the data into test and training
n <- nrow(fire) #number of rows in fire dataset
set.seed(123)
training_rows <- sample(1:n, n*.7, replace=FALSE)
training_sample <- fire[training_rows,]
test_sample <- fire[-training_rows,]

#implement backwards selection
mod1 <- glm(Growth ~ logNumber_of_fire + Period + logIA_Size + sqlogIA_Size + FWI + Method + logResp_time + sqlogResp_time, family = "binomial", data = training_sample)

summary(mod1)

#drop log response time
mod2 <- glm(Growth ~ logNumber_of_fire + Period + logIA_Size + sqlogIA_Size + FWI + Method+ sqlogResp_time, family = "binomial", data = training_sample)


#drop logNumber fires
mod3 <- glm(Growth ~ Period + logIA_Size + sqlogIA_Size + FWI + Method+ sqlogResp_time, family = "binomial", data = training_sample)

summary(mod3)

#drop sqlogIA size
mod4 <- glm(Growth ~ Period + logIA_Size +  FWI + Method+ sqlogResp_time, family = "binomial", data = training_sample)

summary(mod4)

#drop sqlog response time

mod5 <- glm(Growth ~ Period + logIA_Size +  FWI + Method, family = "binomial", data = training_sample)

summary(mod5)

#drop method

mod6 <- glm(Growth ~ Period + logIA_Size +  FWI, family = "binomial", data = training_sample)

summary(mod6)

#drop period
mod7 <- glm(Growth ~  logIA_Size +  FWI, family = "binomial", data = training_sample)

summary(mod7)

preds <- predict(mod7, type = "response", newdata = test_sample)

auc(test_sample$Growth, preds)

plot(roc(test_sample$Growth, preds))


preds_binary <- ifelse(preds > .5,1,0)

confusionMatrix(data=as.factor(preds_binary), reference = as.factor(test_sample$Growth))
```


#Implement Classifcation - Bayesian
```{r}
my_prior <- normal(location = c(-.3, -.05), scale = c(1, 1), autoscale = FALSE)

model_prior <- stan_glm(Growth ~ logIA_Size +  FWI,
                             data = training_sample, family = binomial,
                             prior_intercept = normal(0, 3),
                             prior = my_prior,
                             chains = 4, iter = 5000*2, seed = 84735,
                             prior_PD = TRUE)

model_1 <- update(model_prior, prior_PD = FALSE)

mcmc_trace(model_1)
mcmc_dens_overlay(model_1)
mcmc_acf(model_1)

posterior_interval(model_1, prob = 0.95)
summary(model_1)

#(Intercept)  logIA_Size         FWI 
# 0.92352888 -0.36027309 -0.03301811 


preds_bayes <- predict(model_1, newdata = test_sample, type = "response")

library(pROC)

roc_obj <- roc(test_sample$Growth, preds_bayes)
auc(roc_obj)
#.763

preds_bayes_binary <- ifelse(preds_bayes > .5,1,0)

confusionMatrix(data=as.factor(preds_bayes_binary), reference = as.factor(test_sample$Growth))
```

#Implement Classification - GAM


```{r}
#get default priors
fit_gam <- gam(Growth ~ Period + s(logIA_Size) + s(FWI) + Method + Fuel_type + Detection + s(logResp_time) + Month + s(logNumber_of_fire), family = "binomial", data = training_sample)
summary(fit_gam)
plot(fit_gam)

#predict new
prob_gam1 <- predict(fit_gam, type = "response", newdata = test_sample)

#get auc and roc curves
auc(test_sample$Growth, prob_gam1)

plot(roc(test_sample$Growth, prob_gam1))

#get accuracy
prob_gam1_binary <- ifelse(prob_gam1 > .5,1,0)

confusionMatrix(data=as.factor(prob_gam1_binary), reference = as.factor(test_sample$Growth))

```
```{r}
# get prior
prior.list <- get_prior(Growth ~ Period + s(logIA_Size) + s(FWI) + Method + Fuel_type + Detection + s(logResp_time) + Month + s(logNumber_of_fire), family = "binomial", data = training_sample)




#model_bayes_gam <- brm(
#  bf(Growth ~ Period + s(logIA_Size) + s(FWI) + Method + Fuel_type + Detection + 
#         s(logResp_time) + Month + s(logNumber_of_fire)),
#  family = bernoulli(), 
#  data = training_sample,
#  cores = 8,             
#  iter = 4000,            
#  warmup = 1000,         
#  control = list(adapt_delta = 0.99)
#)



#summary(model_bayes_gam)

#saveRDS(model_bayes_gam, "model_bayes_gam.RDS")
model_bayes_gam <- readRDS("model_bayes_gam.RDS")

prob_gam_bayes <- predict(model_bayes_gam, type = "response", newdata = test_sample)

pred_probs <- predict(model_bayes_gam, newdata = test_sample, type = "response")[, "Estimate"]

library(pROC)

#make plots comparing auc
roc_obj <- roc(test_sample$Growth, pred_probs)
auc(roc_obj)


data.frame(AUC_logistic = 0.7628,
           AUC_logistic_Bayes = 0.763,
           AUC_GAM = 0.7579,
           AUC_GAM_Bayes = 0.7675)


data.frame(logistic = 0.7453,
           logistic_Bayes =  0.7453,
           GAM = 0.7669 ,
           GAM_Bayes = 0.764)

pred_probs_binary <- ifelse(pred_probs > .5,1,0)

confusionMatrix(data=as.factor(pred_probs_binary), reference = as.factor(test_sample$Growth))


```


#Implement Survival Analysis


```{r}
#filter to only growing fires
surv_data <- fire[which(fire[, "Gap"] > 0), ]
```


```{r}
library(flexsurv)

#get the weibull model for cumulative hazard
weimodel <- flexsurvreg(Surv(origin=IA_Size,time= BH_Size, event=Status) ~ FWI + factor(Method) + factor(Fuel_type),
                        data = surv_data, dist = "weibull")



#plot but run the bayesian stuff first to get mean_cumhaz
plot(weimodel, type = "cumhaz", log="x", main = "Cumulative Hazard of Being Held", xlab="Size", ylab= "Cumulative Hazard") 
lines(t_seq, mean_cumhaz, col= "green", type = "l", lwd = 2)
lines(t_seq, lower_cumhaz, col = "green", lty = 2)
lines(t_seq, upper_cumhaz, col = "green", lty = 2)

legend("topleft", 
       legend = c("Posterior Mean", "95% Credible Interval","Weibull Frequentist", "95% Confidence Interval", "Kaplan Meier"), 
       col = c("green", "green", "red", "red", "black"), 
       lty = c(1, 2,1, 2), 
       lwd = c(2, 1, 2, 1),
       bty = "n")

```

```{r}
library(rstan)

stancode_waft <- "
functions {
  // log survival 
  vector log_S(vector t, real shape, vector scale) {
    vector[num_elements(t)] log_S;
    for (i in 1:num_elements(t)) {
      log_S[i] = weibull_lccdf(t[i] | shape, scale[i]);
    }
    return log_S;
  }

  // Log hazard
  vector log_h(vector t, real shape, vector scale) {
    vector[num_elements(t)] log_h;
    vector[num_elements(t)] ls = log_S(t, shape, scale);
    for (i in 1:num_elements(t)) {
      log_h[i] = weibull_lpdf(t[i] | shape, scale[i]) - ls[i];
    }
    return log_h;
  }

  // Log-likelihood for survival data
  real surv_weibull_lpdf(vector t, vector d, real shape, vector scale) {
    vector[num_elements(t)] log_lik;
    vector[num_elements(t)] logS = log_S(t, shape, scale);

    for (i in 1:num_elements(t)) {
      log_lik[i] = d[i] * (weibull_lpdf(t[i] | shape, scale[i])) +
                   (1 - d[i]) * logS[i];
    }

    return sum(log_lik);
  }
}

data {
  int<lower=1> N;                  // num observations
  vector<lower=0>[N] y;           // times
  vector<lower=0, upper=1>[N] event; // censoring indicator
  int<lower=1> M;                 // num covar
  matrix[N, M] x;                 //  covariate matrix
}

parameters {
  vector[M] beta;                 
  real<lower=0> sigma;            
}

transformed parameters {
  vector[N] linpred = x * beta;
  vector[N] mu;
  for (i in 1:N) {
    mu[i] = exp(linpred[i]);
  }
}

model {
  sigma ~ cauchy(0, 25);
  beta ~ normal(0, 10);
  y ~ surv_weibull(event, 1 / sigma, mu);
}

generated quantities {
  vector[N] y_rep;
  vector[N] log_lik;
  for (n in 1:N) {
    real lambda = exp(x[n, ] * beta);
    real shape = 1 / sigma;

    log_lik[n] = event[n] * weibull_lpdf(y[n] | shape, lambda) +
                 (1 - event[n]) * weibull_lccdf(y[n] | shape, lambda);

    y_rep[n] = weibull_rng(shape, lambda);
  }
}

"
#dummy variables
x <- model.matrix(~ FWI + Method + Fuel_type, data = surv_data)

# data
dat1 <- list(
  y = surv_data$BH_Size,
  event = surv_data$Status,
  x = x,
  N = nrow(x),
  M = ncol(x)
)

#get posterior
M1 <- stan(model_code=stancode_waft,data=dat1,
iter=2000,chains=4)

posterior <- extract(M1)

colMeans(posterior[["beta"]])

#get average for plot with log time
x_new <- colMeans(x)  
t_seq <- exp(seq(log(0.01), log(20000), length.out = 500))

n_draws <- length(posterior$sigma)

#get cumulative hazards from posterior
cumhaz_mat <- matrix(NA, nrow = n_draws, ncol = length(t_seq))

for (i in 1:n_draws) {
 shape <- 1 / posterior$sigma[i]
 scale <- exp(sum(posterior$beta[i, ] * x_new))
  cumhaz_mat[i, ] <- (t_seq / scale)^shape
}

#get means and CI
mean_cumhaz <- apply(cumhaz_mat, 2, mean)
lower_cumhaz <- apply(cumhaz_mat, 2, quantile, 0.025)
upper_cumhaz <- apply(cumhaz_mat, 2, quantile, 0.975)

```

```{r}
##get coefs for each model
bayes_coef <- as.data.frame(colMeans(posterior[["beta"]]))
freq_coef <- as.data.frame(coef(weimodel)[-(1)])

colnames(bayes_coef) <- "Bayes"
colnames(freq_coef) <- "Freq"

kable(cbind(freq_coef,bayes_coef),caption = "Model Coefficients/Posterior Means")

plot(M1)
```

```{r}
# dummy vars
x2 <- model.matrix(~ FWI + Method + Fuel_type + Method*number_of_fire, data = surv_data)


dat2 <- list(
  y = surv_data$BH_Size,
  event = surv_data$Status,
  x = x2,
  N = nrow(x2),
  M = ncol(x2)
)

#get posterior
M2 <- stan(model_code=stancode_waft,data=dat2,
iter=2000,chains=4)


posterior2 <- extract(M2)

colMeans(posterior2[["beta"]])

plot(M2)

x_new <- colMeans(x2)  # average 
t_seq <- exp(seq(log(0.01), log(20000), length.out = 500))

n_draws <- length(posterior2$sigma)

#get cumulative hazard again
cumhaz_mat <- matrix(NA, nrow = n_draws, ncol = length(t_seq))

for (i in 1:n_draws) {
shape <- 1 / posterior2$sigma[i]
 scale <- exp(sum(posterior2$beta[i, ] * x_new))
  cumhaz_mat[i, ] <- (t_seq / scale)^shape
}

mean_cumhaz <- apply(cumhaz_mat, 2, mean)
lower_cumhaz <- apply(cumhaz_mat, 2, quantile, 0.025)
upper_cumhaz <- apply(cumhaz_mat, 2, quantile, 0.975)

#######PLOT
plot(weimodel, type = "cumhaz", log="x", main = "Cumulative Hazard of Being Held", xlab="Size", ylab= "Cumulative Hazard") 
lines(t_seq, mean_cumhaz, col= "green", type = "l", lwd = 2)
lines(t_seq, lower_cumhaz, col = "green", lty = 2)
lines(t_seq, upper_cumhaz, col = "green", lty = 2)

legend("topleft", 
       legend = c("Posterior Mean", "95% Credible Interval","Weibull Frequentist", "95% Confidence Interval", "Kaplan Meier"), 
       col = c("green", "green", "red", "red", "black"), 
       lty = c(1, 2,1, 2), 
       lwd = c(2, 1, 2, 1),
       bty = "n")


#########TABLE

bayes_coef2 <- as.data.frame(colMeans(posterior2[["beta"]]))
rownames(bayes_coef2) <- colnames(x2)

coef2l <- apply(posterior2[["beta"]], 2, quantile, 0.025)
coef2u <- apply(posterior2[["beta"]], 2, quantile, 0.975)

mod2res <- cbind(bayes_coef2, coef2l)
mod2res <- cbind(mod2res, coef2u)
```
```{r}
#get the cumulative hazard curves for each method using the average results for the rest of the columns

x_newair1 <- colMeans(x2)  
x_newair1[3:6] <- c(0,0,0,0)
x_newair1[11:14] <- c(0,0,0,0)
t_seq <- exp(seq(log(0.01), log(20000), length.out = 500))
n_draws <- length(posterior2$sigma)
cumhaz_mat <- matrix(NA, nrow = n_draws, ncol = length(t_seq))
for (i in 1:n_draws) {
shape <- 1 / posterior2$sigma[i]
 scale <- exp(sum(posterior2$beta[i, ] * x_newair1))
  cumhaz_mat[i, ] <- (t_seq / scale)^shape
}
mean_cumhazair1 <- apply(cumhaz_mat, 2, mean)


x_newair <- colMeans(x2) 
x_newair[3:6] <- c(1,0,0,0)
x_newair[11:14] <- c(x_newair[10],0,0,0)
t_seq <- exp(seq(log(0.01), log(20000), length.out = 500))
n_draws <- length(posterior2$sigma)
cumhaz_mat <- matrix(NA, nrow = n_draws, ncol = length(t_seq))
for (i in 1:n_draws) {
shape <- 1 / posterior2$sigma[i]
 scale <- exp(sum(posterior2$beta[i, ] * x_newair))
  cumhaz_mat[i, ] <- (t_seq / scale)^shape
}
mean_cumhazair <- apply(cumhaz_mat, 2, mean)


x_newground <- colMeans(x2)  
x_newground[3:6] <- c(0,1,0,0)
x_newground[11:14] <- c(0,x_newground[10],0,0)
t_seq <- exp(seq(log(0.01), log(20000), length.out = 500))
n_draws <- length(posterior2$sigma)
cumhaz_mat <- matrix(NA, nrow = n_draws, ncol = length(t_seq))
for (i in 1:n_draws) {
shape <- 1 / posterior2$sigma[i]
 scale <- exp(sum(posterior2$beta[i, ] * x_newground))
  cumhaz_mat[i, ] <- (t_seq / scale)^shape
}
mean_cumhazground <- apply(cumhaz_mat, 2, mean)


x_newh <- colMeans(x2) 
x_newh[3:6] <- c(0,0,1,0)
x_newh[11:14] <- c(0,0,x_newh[10],0)
t_seq <- exp(seq(log(0.01), log(20000), length.out = 500))
n_draws <- length(posterior2$sigma)
cumhaz_mat <- matrix(NA, nrow = n_draws, ncol = length(t_seq))
for (i in 1:n_draws) {
shape <- 1 / posterior2$sigma[i]
 scale <- exp(sum(posterior2$beta[i, ] * x_newh))
  cumhaz_mat[i, ] <- (t_seq / scale)^shape
}
mean_cumhazh <- apply(cumhaz_mat, 2, mean)


x_newo <- colMeans(x2)  
x_newo[3:6] <- c(0,0,0,1)
x_newo[11:14] <- c(0,0,0,x_newo[10])
t_seq <- exp(seq(log(0.01), log(20000), length.out = 500))
n_draws <- length(posterior2$sigma)
cumhaz_mat <- matrix(NA, nrow = n_draws, ncol = length(t_seq))
for (i in 1:n_draws) {
shape <- 1 / posterior2$sigma[i]
 scale <- exp(sum(posterior2$beta[i, ] * x_newo))
  cumhaz_mat[i, ] <- (t_seq / scale)^shape
}
mean_cumhazo <- apply(cumhaz_mat, 2, mean)



# whoops i want survival....
mean_surv_air1 <- exp(-mean_cumhazair)
mean_surv_air <- exp(-mean_cumhazair)
mean_surv_ground <- exp(-mean_cumhazground)
mean_surv_h <- exp(-mean_cumhazh)
mean_surv_o <- exp(-mean_cumhazo)

# Plot
plot(x = t_seq, y = mean_surv_air, log = "x",
     main = "Probability of Growing by Method",
     xlab = "Size", ylab = "Probability of Growing",
     type = "l", lwd = 2, col = "red", ylim = c(0, 1))
lines(t_seq, mean_surv_air1, col = "orange", type = "l", lwd = 2)
lines(t_seq, mean_surv_ground, col = "blue", type = "l", lwd = 2)
lines(t_seq, mean_surv_h, col = "green", type = "l", lwd = 2)
lines(t_seq, mean_surv_o, col = "brown", type = "l", lwd = 2)

legend("topright",
       legend = c("Air tanker", "Air", "Ground trained", "Helitanker", "Other ground"),
       col = c("red","orange","blue", "green", "brown"),
       lty = 1, lwd = 2, bty = "n")

```
```{r}
###traceplots

traceplot(M2)
```




Arienti, M. C., Cumming, S. G., & Boutin, S. (2006). Empirical models of forest fire initial attack success probabilities: the effects of fuels, anthropogenic linear features, fire weather, and management. Canadian Journal of Forest Research, 36(12), 3155-3166.

Bowman, D. M., Williamson, G. J., Abatzoglou, J. T., Kolden, C. A., Cochrane, M. A., & Smith, A. M. (2017). Human exposure and sensitivity to globally extreme wildfire events. Nature ecology & evolution, 1(3), 0058.

Bürkner P (2017). “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software, 80(1), 1–28. doi:10.18637/jss.v080.i01.

“Canadian Wildland Fire Glossary.” Canadian Wildland Fire Glossary, CIFFC Training Working Group, 16 Mar. 2012, ciffc.ca/sites/default/files/2022-03/CWFM_glossary_EN.pdf. 

Cumming, S. G. (2001). A parametric model of the fire-size distribution. Canadian Journal of Forest Research, 31(8), 1297-1303.

Cunningham, C. X., Williamson, G. J., & Bowman, D. M. (2024). Increasing frequency and intensity of the most extreme wildfires on Earth. Nature ecology & evolution, 8(8), 1420-1425.

Goodrich B, Gabry J, Ali I & Brilleman S. (2024). rstanarm: Bayesian applied
regression modeling via Stan. R package version 2.32.1 https://mc-stan.org/rstanarm.

Kilkenny, K., Frishman, W., & Alpert, J. S. (2025). Los Angeles Wildfires: Getting to the Heart of It. The American Journal of Medicine.

Martell, D. L., & Sun, H. (2008). The impact of fire suppression, vegetation, and weather on the area burned by lightning-caused forest fires in Ontario. Canadian Journal of Forest Research, 38(6), 1547-1563.

Miller, D. L. (2025). Bayesian views of generalized additive modelling. Methods in Ecology and Evolution, 16(3), 446-455.

Morin, A. A., Albert-Green, A., Woolford, D. G., & Martell, D. L. (2015). The use of survival analysis methods to model the control time of forest fires in Ontario, Canada. International Journal of Wildland Fire, 24(7), 964-973.

Strauss, D., Bednar, L., & Mees, R. (1989). Do one percent of the forest fires cause ninety-nine percent of the damage?. Forest science, 35(2), 319-328.

Tremblay, P. O., Duchesne, T., & Cumming, S. G. (2018). Survival analysis and classification methods for forest fire size. PLoS one, 13(1), e0189860.

Van De Schoot, R., Broere, J. J., Perryck, K. H., Zondervan-Zwijnenburg, M., & Van Loey, N. E. (2015). Analyzing small data sets using Bayesian estimation: The case of posttraumatic stress symptoms following mechanical ventilation in burn survivors. European journal of psychotraumatology, 6(1), 25216.

White, L. F., Jiang, W., Ma, Y., So-Armah, K., Samet, J. H., & Cheng, D. M. (2020). Tutorial in biostatistics: the use of generalized additive models to evaluate alcohol consumption as an exposure variable. Drug and alcohol dependence, 209, 107944.



